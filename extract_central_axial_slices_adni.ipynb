{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f4dcd4c",
   "metadata": {},
   "source": [
    "# Transfer Learning - VGG16 Model\n",
    "\n",
    "\n",
    "- Alzhemimer's Disease Neuroimaging Initiative (ADNI) dataset\n",
    "- Three classes:\n",
    "    - Normal control\n",
    "    - Mild cognitive impairment\n",
    "    - Alzheimer's disease\n",
    "- 6 hospitals\n",
    "\n",
    "* Paths to AD and CN classes must be set before running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ae8e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2cd2aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def zoom_out(image, width=128, height=128):\n",
    "    new_img = zoom(image, (width / image.shape[0], height / image.shape[1],1))\n",
    "    return new_img\n",
    "\n",
    "def read_nifti_file(filepath):\n",
    "    scan = nib.load(filepath)\n",
    "    scan = scan.get_fdata()\n",
    "    if scan.ndim == 3:\n",
    "        scan = scan[:,:,:,np.newaxis]\n",
    "    return scan\n",
    "\n",
    "def normalize(volume):\n",
    "    volume = 255*(volume - volume.min()) / (volume.max() - volume.min())\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "def process_scan(path,nslices, H, W):\n",
    "    volume = read_nifti_file(path)\n",
    "    volume = normalize(volume)\n",
    "    volume = np.concatenate((volume[:-2],volume[1:-1], volume[2:]), axis = -1)\n",
    "    volume_slices = np.zeros((nslices,H,W,volume.shape[3]),dtype = np.float32)\n",
    "    for (ii,jj) in enumerate(range(int(volume.shape[2] / 2 - nslices/2),int(volume.shape[2] / 2 + nslices/2))): \n",
    "        volume_slices[ii] = zoom_out(volume[:,:,jj,:], H, W)\n",
    "    #volume_slices = normalize(volume_slices)\n",
    "    return volume_slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf3e9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "1129\n"
     ]
    }
   ],
   "source": [
    "# Listing AD and NC subjects/volumes\n",
    "root = '/path/to/AD'\n",
    "ad_paths = []\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    aux = path\n",
    "    for name in files:\n",
    "        if '.nii' in name:\n",
    "            if name[0] != '.':\n",
    "                ad_paths.append(aux + '/' + name)\n",
    "\n",
    "root = '/path/to/CN'\n",
    "nc_paths = []\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    aux = path\n",
    "    for name in files:\n",
    "        if '.nii' in name:\n",
    "            if name[0] != '.':\n",
    "                nc_paths.append(aux + '/' + name)\n",
    "                \n",
    "print(len(ad_paths))       \n",
    "print(len(nc_paths)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece839e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nslices = 10 # Slices per volume to extract\n",
    "H, W = 256, 256 # Resize dimensions\n",
    "class_names = [\"AD\", \"NC\"]\n",
    "\n",
    "all_paths = ad_paths + nc_paths \n",
    "y_subjects = [0]*len(ad_paths) + [1]*len(nc_paths) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abe17ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train/val/test sets\n",
    "paths_train, paths_test, y_subjects_train, y_subjects_test = \\\n",
    "train_test_split(all_paths, y_subjects,test_size=0.2, random_state=0,stratify =y_subjects)\n",
    "\n",
    "paths_train, paths_val, y_subjects_train, y_subjects_val = \\\n",
    "train_test_split(paths_train, y_subjects_train, test_size=0.2, random_state=0,stratify =y_subjects_train)\n",
    "\n",
    "hospitals = ['GE_15', 'GE_3', 'Philips_15', 'Philips_3' ,'Siemens_15', 'Siemens_3']\n",
    "\n",
    "paths_train = np.array([f.split(\"\\\\\")[6] for f in paths_train])\n",
    "paths_val = np.array([f.split(\"\\\\\")[6] for f in paths_val])\n",
    "paths_test = np.array([f.split(\"\\\\\")[6] for f in paths_test])\n",
    "\n",
    "center_train = np.zeros(paths_train.size, dtype = int) \n",
    "center_val = np.zeros(paths_val.size, dtype = int)\n",
    "center_test = np.zeros(paths_test.size, dtype = int)\n",
    "\n",
    "\n",
    "for (counter,ii) in enumerate(hospitals):\n",
    "    center_train[paths_train == ii] = counter\n",
    "    center_val[paths_val == ii] = counter\n",
    "    center_test[paths_test == ii] = counter\n",
    "    \n",
    "center_train = np.repeat(center_train,nslices)\n",
    "center_val = np.repeat(center_val,nslices)\n",
    "center_test = np.repeat(center_test,nslices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1c5185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((nslices*len(paths_train),H,W,3), dtype = np.float32)\n",
    "Y_train = np.repeat(y_subjects_train,nslices)\n",
    "\n",
    "counter = 0\n",
    "for img_file in paths_train:\n",
    "    temp_img = process_scan(img_file,nslices, H, W)\n",
    "    X_train[counter*nslices:(counter+1)*nslices] = temp_img\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16651f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.zeros((nslices*len(paths_val),H,W,3), dtype = np.float32)\n",
    "Y_val = np.repeat(y_subjects_val,nslices)\n",
    "\n",
    "counter = 0\n",
    "for img_file in paths_val:\n",
    "    temp_img = process_scan(img_file,nslices, H, W)\n",
    "    X_val[counter*nslices:(counter+1)*nslices] = temp_img\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ece5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.zeros((nslices*len(paths_test),H,W,3), dtype = np.float32)\n",
    "Y_test = np.repeat(y_subjects_test,nslices)\n",
    "\n",
    "counter = 0\n",
    "for img_file in paths_test:\n",
    "    temp_img = process_scan(img_file,nslices, H, W)\n",
    "    X_test[counter*nslices:(counter+1)*nslices] = temp_img\n",
    "    counter+=1\n",
    "print(X_test.min(),X_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d67a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "Y_train_oh = tf.keras.utils.to_categorical(Y_train, 2)\n",
    "Y_val_oh = tf.keras.utils.to_categorical(Y_val, 2)\n",
    "Y_test_oh = tf.keras.utils.to_categorical(Y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './adni/'\n",
    "\n",
    "np.save(root + \"X_train_ax.npz\",X_train)\n",
    "np.save(root + \"X_val_ax.npz\",X_val)\n",
    "np.save(root + \"X_test_ax.npz\",X_test)\n",
    "\n",
    "np.save(root + \"Y_train_ax.npz\",Y_train)\n",
    "np.save(root + \"Y_val_ax.npz\",Y_val)\n",
    "np.save(root + \"Y_test_ax.npz\",Y_test)\n",
    "\n",
    "np.save(root + \"center_train\",center_train)\n",
    "np.save(root + \"center_val\",center_val)\n",
    "np.save(root + \"center_test\",center_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}